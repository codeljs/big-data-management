{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf88d94",
   "metadata": {},
   "source": [
    "----------\n",
    "## Notebook setup\n",
    "\n",
    "If this notebook is using the \"PySpark\" kernel and you have setup Livy using SSH, you can now access the cluster.\n",
    "\n",
    "Everytime you run a cell, your web browser window title will show a **(Busy)** status along with the notebook title. You will also see a solid circle next to the **PySpark** text in the top-right corner. After the job completes, this will change to a hollow circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe423ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99e103e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tError sending http request and maximum retry encountered..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "print(\"Running a simple command to start connection to spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b76dc",
   "metadata": {},
   "source": [
    "### Session information (%%info)\n",
    "\n",
    "Livy is an open source REST server for Spark. When you execute a code cell in a PySpark notebook, it creates a Livy session to execute your code. You can use the `%%info` magic to display the current Livy session information. Magic commands start with %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "111683b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tFailed to register auto viz for notebook.\n",
      "Exception details:\n",
      "\t\"cannot import name 'DataError' from 'pandas.core.groupby' (/opt/homebrew/anaconda3/envs/sparkmagicEnv/lib/python3.8/site-packages/pandas/core/groupby/__init__.py)\".\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed828ee",
   "metadata": {},
   "source": [
    "Showing all avaliable \"sparkmagic\" commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97369150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tFailed to register auto viz for notebook.\n",
      "Exception details:\n",
      "\t\"cannot import name 'DataError' from 'pandas.core.groupby' (/opt/homebrew/anaconda3/envs/sparkmagicEnv/lib/python3.8/site-packages/pandas/core/groupby/__init__.py)\".\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "%%help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1d8451",
   "metadata": {},
   "source": [
    "----------\n",
    "## PySpark magics \n",
    "\n",
    "The PySpark kernel provides some predefined “magics”, which are special commands that you can call with `%%` (e.g. `%%MAGIC` <args>). The magic command must be the first word in a code cell and allow for multiple lines of content. You can’t put comments before a cell magic.\n",
    "\n",
    "For more information on magics, see [here](http://ipython.readthedocs.org/en/stable/interactive/magics.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3e43e1",
   "metadata": {},
   "source": [
    "SparkSession available as the variable called 'spark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed2a3ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tFailed to register auto viz for notebook.\n",
      "Exception details:\n",
      "\t\"cannot import name 'DataError' from 'pandas.core.groupby' (/opt/homebrew/anaconda3/envs/sparkmagicEnv/lib/python3.8/site-packages/pandas/core/groupby/__init__.py)\".\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94d3cbf",
   "metadata": {},
   "source": [
    "Let's investigate its type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581cf1f4",
   "metadata": {},
   "source": [
    "Normal python code can be executed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a907b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9997c600",
   "metadata": {},
   "source": [
    "We can also import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d9ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d6aaa2",
   "metadata": {},
   "source": [
    "Let's run a spark function using the spark variable. Use %%pretty magic command to show the dataframe nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pretty\n",
    "# Read the csv-file from HDFS\n",
    "df = spark.read\\\n",
    "    .option(\"header\",True)\\\n",
    "    .csv(\"/datasets/retail/retail.csv\")\n",
    "\n",
    "# Show the first 20 rows as text\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38dff92",
   "metadata": {},
   "source": [
    "Note this dataframe is not a pandas dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a95fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f7ef79",
   "metadata": {},
   "source": [
    "How many rows do we have access to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd4ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:,.0f} rows in df\".format( df.count() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f647c48",
   "metadata": {},
   "source": [
    "Let's save resources by only working on a subset (10%) of the dataframe, while we develop our code. This makes calculations faster for you, and everyone else using the cluster.\n",
    "When we are sure our code works as intended, we can delete this code. We use: \n",
    "\n",
    "``DataFrame.sample(withReplacement=None, fraction=None, seed=None)``\n",
    "See https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.sample.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739a99f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(withReplacement=False, fraction=1/10)\n",
    "\n",
    "print(\"{:,.0f} rows in df\".format( df.count() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c3ce01",
   "metadata": {},
   "source": [
    "### SQL magic (%%sql)\n",
    "\n",
    "The PySpark kernel supports easy inline SparkSQL queries against the `sqlContext`, which is needed for some part of the assignment. So if you are comfortable with SQL, you can create a temporary view on DataFrame/Dataset by using createOrReplaceTempView() and using SQL to select and manipulate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e50ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"invoices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf2b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT InvoiceNo, SUM(Quantity) AS total_quantity \n",
    "FROM invoices \n",
    "GROUP BY InvoiceNo \n",
    "ORDER BY total_quantity DESC \n",
    "LIMIT 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70844fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT DISTINCT(Country) \n",
    "FROM invoices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f37feb1",
   "metadata": {},
   "source": [
    "Using pyspark, answer the following questions (but first see notes below):  \n",
    "1. What is the average UnitPrice in retail.csv?  \n",
    "1. What is the data type of each column (schema) in this file?  \n",
    "1. How to find the number of unique countries using DataFrames instead of SparkSQL?\n",
    "1. How many invoices are from Japan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e00d0bf",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f34772",
   "metadata": {},
   "source": [
    "----\n",
    "### Optional: Making code run faster: Session configuration (%%configure)\n",
    "\n",
    "**NOTE: Many students use the clusters resources. Please be aware of how many resources you use, to allow enough for all!**\n",
    "\n",
    "All students can view which applications are running on the cluster, and how many resources people use at the <a href=\"http://130.226.142.166:8088/cluster/scheduler\" target=\"_blank\"> cluster scheduler overview</a>. When connecting through \"Livy\" you cannot see the username of the student.\n",
    "\n",
    "Use the `%%configure` magic to configure new or existing Livy sessions.\n",
    "* If a session is already running, you can change the configuration by using the `-f` argument with `%%configure` magic. This will delete the current session and recreate it with the applied configurations. If you don't provide the `-f` argument, an error will be displayed and no configuration changes will be applied.\n",
    "* If you haven't already started the session, then the `-f` argument is not mandatory. Even if you use it with a session that you are just creating, it will not delete any currently running sessions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "These are some session attributes that can be used for configuration \n",
    "- **\"name\"**: Name of the application\n",
    "- **\"driverMemory\"**: Memory for driver (e.g. 1000M, 2G) \n",
    "- **\"executorMemory\"**: Memory for executor (e.g. 1000M, 2G) \n",
    "\n",
    "For more attributes for session configuration see <a href=\"https://github.com/cloudera/livy/tree/6fe1e80cfc72327c28107e0de20c818c1f13e027#post-sessions\" target=\"_blank\"> the Livy documentation</a>.\n",
    "\n",
    "> **TIP**: The application name should start with `remotesparkmagics` to allow sessions to get automatically cleaned up if an error happened. If you provide a name that does not start with `remotesparkmagics` it will not result in an error but the cleanup won't occur.\n",
    "\n",
    "\n",
    "By default the PySpark shell will be allocated a modest amount of resources on the cluster, but you can specify this through the  options.\n",
    "\n",
    "\\begin{lstlisting}\n",
    "--name NAME                 A name of your application.\n",
    "--conf PROP=VALUE           Arbitrary Spark configuration property.\n",
    "--driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n",
    "--executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n",
    "--driver-cores NUM          Number of cores used by the driver, only in cluster mode (Default: 1).\n",
    "--executor-cores NUM        Number of cores per executor. (Default: 1)\n",
    "--num-executors NUM         Number of executors to launch (Default: 2).\n",
    "\\end{lstlisting}\n",
    "\n",
    "For example if you want a pyspark session with 4 executors, with 4 gigabytes of memory each, you would write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a32fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f \n",
    "{\"name\":\"remotesparkmagics-sample\", \"executorMemory\": \"4G\", \"numExecutors\":4}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26fa455",
   "metadata": {},
   "source": [
    "Read more about sparkmagic here in the original notebook:\n",
    "https://github.com/jupyter-incubator/sparkmagic/blob/master/examples/Pyspark%20Kernel.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
